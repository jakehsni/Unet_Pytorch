{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_utils import get_param_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1_bn_relu(inp_activation, output_activation, BN=True, activation = True):\n",
    "    \"con 1x1 + Batchnormalization + relu\"\n",
    "    layer = [nn.Conv2d(inp_activation, output_activation, 1)]\n",
    "    for i, j in zip([nn.BatchNorm2d(output_activation), nn.ReLU(inplace=True)],[BN, activation]):\n",
    "        if j==True:\n",
    "            layer.append(i)\n",
    "    return nn.Sequential(*layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3_bn_relu(inp_activation, output_activation, BN=True, activation = True):\n",
    "    \"con 3x3 + Batchnormalization + relu\"\n",
    "    layer = [nn.Conv2d(inp_activation, output_activation, 3, padding = 1)]\n",
    "    for i, j in zip([nn.BatchNorm2d(output_activation), nn.ReLU(inplace=True)],[BN, activation]):\n",
    "        if j==True:\n",
    "            layer.append(i)\n",
    "    return nn.Sequential(*layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(23, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3x3_bn_relu(23,34,BN = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv5x5_bn_relu(inp_activation, output_activation, BN = True, activation = True):\n",
    "    \"con 5x5 + Batchnormalization + relu\"\n",
    "    layer = [nn.Conv2d(inp_activation, output_activation, 5, padding = 2)]\n",
    "    for i, j in zip([nn.BatchNorm2d(output_activation), nn.ReLU(inplace=True)], [BN, activation]):\n",
    "        if j==True:\n",
    "            layer.append(i)\n",
    "    return nn.Sequential(*layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bottleneck_block(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_activation, list_filter=[256, 64, 256], down = None):\n",
    "        super().__init__()\n",
    "        self.conv1x1_1 = conv1x1_bn_relu(input_activation, list_filter[0])\n",
    "        self.conv3x3 = conv3x3_bn_relu(list_filter[0], list_filter[1])\n",
    "        self.conv1x1_2 = conv1x1_bn_relu(list_filter[1], list_filter[2])\n",
    "        self.down = down\n",
    "        if down!=None:\n",
    "            self.contract_conv = down\n",
    "    def forward(self , inp):\n",
    "        x = inp\n",
    "        c = self.conv1x1_1(inp)\n",
    "        c = self.conv3x3(c)\n",
    "        result = self.conv1x1_2(c)\n",
    "        if self.down!=None:\n",
    "            x = self.contract_conv(inp)\n",
    "        \n",
    "        out = result + x\n",
    "        \n",
    "        return out\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp_activation, list_filter, BN = False):\n",
    "        super().__init__()\n",
    "        self.conv3x3_1 = conv3x3_bn_relu(inp_activation, list_filter, BN=BN)\n",
    "        self.conv3x3_2 = conv3x3_bn_relu(list_filter, list_filter, BN=BN)\n",
    "    def forward(self , inp):\n",
    "        c = self.conv3x3_1(inp)\n",
    "        c = self.conv3x3_2(c)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_block(\n",
       "  (conv3x3_1): Sequential(\n",
       "    (0): Conv2d(23, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv3x3_2): Sequential(\n",
       "    (0): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_block(23,34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block_bottle(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp_activation, output):\n",
    "        super().__init__()\n",
    "        self.input = inp_activation\n",
    "        self.output = output\n",
    "        \n",
    "        self.bottle1 = bottleneck_block(inp_activation, [inp_activation]*3)\n",
    "            \n",
    "        self.bottle2 = bottleneck_block(inp_activation, [inp_activation]*2 + [output], down  = nn.Conv2d(inp_activation\n",
    "                                                                                                       ,output, 1))\n",
    "\n",
    "    def forward(self , inp):\n",
    "        c = self.bottle1(inp)\n",
    "        c = self.bottle2(c)\n",
    "        \n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet_res_b(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.en_block1 = conv_block(16,32)\n",
    "        self.en_block2 = conv_block(32,64)\n",
    "        self.en_block3 = conv_block(64,128)\n",
    "        self.en_block4 = conv_block(128,256)\n",
    "        self.en_block5 = conv_block(256,512)\n",
    "        self.en_block6 = conv_block(512, 1024)\n",
    "\n",
    "        \n",
    "        self.transpose5 = nn.ConvTranspose2d(1024,512,2,2)\n",
    "        self.transpose4 = nn.ConvTranspose2d(512,256,2,2)\n",
    "\n",
    "        self.transpose3 = nn.ConvTranspose2d(256,128,2,2)\n",
    "        self.transpose2 = nn.ConvTranspose2d(128,64,2,2)\n",
    "        self.transpose1 = nn.ConvTranspose2d(64,32,2,2)\n",
    "        \n",
    "        self.de_block1 = conv_block(64,32)\n",
    "        self.de_block2 = conv_block(128,64)\n",
    "        self.de_block3 = conv_block(256,128)\n",
    "\n",
    "        self.de_block4 = conv_block(512, 256)\n",
    "        self.de_block5 = conv_block(1024, 512)\n",
    "        self.out_conv = nn.Conv2d(32, n_class, 1)\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, inp):\n",
    "        el1 = self.en_block1(inp) #  (32,h,w)\n",
    "        print('el1',el1.shape)\n",
    "        max1 = nn.MaxPool2d(2)(el1) # (32,h//2, w//2)\n",
    "        print('max1',max1.shape)\n",
    "\n",
    "        el2 = self.en_block2(max1)    #(64, h//2, w//2)\n",
    "        print('el2',el2.shape)\n",
    "\n",
    "        max2 = nn.MaxPool2d(2)(el2)  #(64, h//4, w//4)\n",
    "        print('max2',max2.shape)\n",
    "\n",
    "        el3 = self.en_block3(max2)    #(128, h//4, w//4)\n",
    "        print('el3',el3.shape)\n",
    "\n",
    "        max3 = nn.MaxPool2d(2)(el3)  #(128, h//8, w//8)\n",
    "        print('max3',max3.shape)\n",
    "\n",
    "\n",
    "        el4 = self.en_block4(max3)    #(256, h//8, w//8)\n",
    "        print('el4',el4.shape)\n",
    "\n",
    "        max4 = nn.MaxPool2d(2)(el4)  #(256, h//16, w//16)\n",
    "        print('max4',max4.shape)\n",
    "\n",
    "        el5 = self.en_block5(max4)  #(512, h//16, w//16)\n",
    "        print('el5',el5.shape)\n",
    "\n",
    "        max5 = nn.MaxPool2d(2)(el5)  #(512, h//32, w//32)\n",
    "        print('max5',max5.shape)\n",
    "\n",
    "        \n",
    "        el6 = self.en_block6(max5)  #(1024, h//32, w//32)\n",
    "        print('el6',el6.shape)\n",
    "\n",
    "\n",
    "        tl5 = self.transpose5(el6)  #(512, h//16, w//16)\n",
    "        print('tl5',tl5.shape)\n",
    "\n",
    "        cat5 = torch.cat([tl5, el5], 1) #(1024, h//16, h//16 )\n",
    "        print('cat5',cat5.shape)\n",
    "\n",
    "        d5 =  self.de_block5(cat5)      #(512, h//16, w//16\n",
    "        print('d5',d5.shape)\n",
    "\n",
    "        \n",
    "        tl4 = self.transpose4(d5)       #(256, h//8, w//8)\n",
    "        cat4 = torch.cat([tl4, el4], 1) #(512, h//8, w//8)\n",
    "        d4 =  self.de_block4(cat4)     #(256, h//8, w//8)\n",
    "        \n",
    "        tl3 = self.transpose3(d4)        #(128, h//4, w//4)\n",
    "        cat3 = torch.cat([tl3, el3], 1)  #(256, h//4, w//4)\n",
    "        d3 =  self.de_block3(cat3)        #(128, h//4, w//4)\n",
    "        \n",
    "        \n",
    "        tl2 = self.transpose2(d3)          #(64, h//2, w//2)\n",
    "        cat2 = torch.cat([tl2, el2], 1)   #(128, h//2, w//2)\n",
    "        d2 =  self.de_block2(cat2)         #(64, h//2, w//2)\n",
    "        \n",
    "        tl1 = self.transpose1(d2)          #(32, h, w)\n",
    "        cat1 = torch.cat([tl1, el1], 1) #(64, h, w)\n",
    "        d1 =  self.de_block1(cat1)        #(32, h, w)\n",
    "        output = self.out_conv(d1) \n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Unet_res_b(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,16,192,192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4608\n",
      "32\n",
      "9216\n",
      "32\n",
      "18432\n",
      "64\n",
      "36864\n",
      "64\n",
      "73728\n",
      "128\n",
      "147456\n",
      "128\n",
      "294912\n",
      "256\n",
      "589824\n",
      "256\n",
      "1179648\n",
      "512\n",
      "2359296\n",
      "512\n",
      "4718592\n",
      "1024\n",
      "9437184\n",
      "1024\n",
      "2097152\n",
      "512\n",
      "524288\n",
      "256\n",
      "131072\n",
      "128\n",
      "32768\n",
      "64\n",
      "8192\n",
      "32\n",
      "18432\n",
      "32\n",
      "9216\n",
      "32\n",
      "73728\n",
      "64\n",
      "36864\n",
      "64\n",
      "294912\n",
      "128\n",
      "147456\n",
      "128\n",
      "1179648\n",
      "256\n",
      "589824\n",
      "256\n",
      "4718592\n",
      "512\n",
      "2359296\n",
      "512\n",
      "64\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in m.parameters():\n",
    "    print(i.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el1 torch.Size([1, 32, 192, 192])\n",
      "max1 torch.Size([1, 32, 96, 96])\n",
      "el2 torch.Size([1, 64, 96, 96])\n",
      "max2 torch.Size([1, 64, 48, 48])\n",
      "el3 torch.Size([1, 128, 48, 48])\n",
      "max3 torch.Size([1, 128, 24, 24])\n",
      "el4 torch.Size([1, 256, 24, 24])\n",
      "max4 torch.Size([1, 256, 12, 12])\n",
      "el5 torch.Size([1, 512, 12, 12])\n",
      "max5 torch.Size([1, 512, 6, 6])\n",
      "el6 torch.Size([1, 1024, 6, 6])\n",
      "tl5 torch.Size([1, 512, 12, 12])\n",
      "cat5 torch.Size([1, 1024, 12, 12])\n",
      "d5 torch.Size([1, 512, 12, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.3366e-02,  1.1545e-02,  9.4780e-03,  ...,  1.1809e-02,\n",
       "            1.1548e-02,  1.6960e-02],\n",
       "          [ 3.4588e-03, -1.7204e-03,  6.4900e-04,  ...,  3.4514e-03,\n",
       "            3.8509e-03,  1.5615e-02],\n",
       "          [-9.5516e-06, -2.8742e-03,  5.8421e-04,  ..., -2.7326e-03,\n",
       "            6.8820e-03,  1.5244e-02],\n",
       "          ...,\n",
       "          [ 1.6847e-03, -3.3051e-03, -3.0177e-03,  ...,  4.1054e-03,\n",
       "            1.3878e-03,  1.1440e-02],\n",
       "          [-1.9456e-04,  9.8195e-04, -4.2372e-04,  ...,  2.5283e-03,\n",
       "            6.5405e-03,  1.1661e-02],\n",
       "          [ 8.6760e-03,  6.9916e-03,  6.3358e-03,  ...,  8.5623e-03,\n",
       "            9.8536e-03,  1.2641e-02]],\n",
       "\n",
       "         [[-3.2431e-02, -3.8877e-02, -4.0658e-02,  ..., -3.6740e-02,\n",
       "           -3.3202e-02, -3.1175e-02],\n",
       "          [-2.6763e-02, -3.9406e-02, -3.7323e-02,  ..., -3.5664e-02,\n",
       "           -3.3962e-02, -3.2038e-02],\n",
       "          [-2.6033e-02, -3.5824e-02, -3.3835e-02,  ..., -3.4929e-02,\n",
       "           -3.3021e-02, -3.2600e-02],\n",
       "          ...,\n",
       "          [-2.6826e-02, -3.4777e-02, -3.3504e-02,  ..., -2.8637e-02,\n",
       "           -3.5252e-02, -3.2186e-02],\n",
       "          [-2.6382e-02, -3.0659e-02, -3.1691e-02,  ..., -2.9320e-02,\n",
       "           -3.1610e-02, -3.0675e-02],\n",
       "          [-2.0598e-02, -2.4297e-02, -2.3526e-02,  ..., -2.7194e-02,\n",
       "           -2.5575e-02, -2.8626e-02]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, grad_clip, loss):\n",
    "    \"train model\"\n",
    "    for batch in dataloader:\n",
    "        model(batch)\n",
    "        loss = loss\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if grad_clip!= None:\n",
    "            nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (other-env)",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
